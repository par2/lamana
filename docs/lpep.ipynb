{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "nbsphinx": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Run: 2016-09-14 19:38:27\n"
     ]
    }
   ],
   "source": [
    "# Hidden TimeStamp\n",
    "import time, datetime\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print('Last Run: {}'.format(st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "## LPEP #:  [\\Template Title]\n",
    "\n",
    "\n",
    "- **Status: ![Review Pathway](_images/pep paths.png)**\n",
    "- **Type: Standards Track, Informational, Process**\n",
    "- **Date: **\n",
    "- **Current Version: **\n",
    "\n",
    "\n",
    "### Motivation\n",
    "\n",
    "\n",
    "\n",
    "### Desired Ouptut\n",
    "\n",
    "\n",
    "\n",
    "### Definitions/Keywords\n",
    "\n",
    "\n",
    "\n",
    "### Specification\n",
    "\n",
    "\n",
    "\n",
    "### Examples\n",
    "\n",
    "\n",
    "\n",
    "### Vetting\n",
    "\n",
    "\n",
    "\n",
    "### Next Actions\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "### See Also\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "### Copyright \n",
    "\n",
    "This document has been placed in the public domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LPEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LamAna Python Enhancement Proposals (LPEP) and Micro PEPs."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. seealso ::\n",
    "\n",
    "    The LPEP `types <https://www.python.org/dev/peps/pep-0001/#pep-types>`_, `submission <https://www.python.org/dev/peps/pep-0001/#id29>`_ and `content <https://www.python.org/dev/peps/pep-0001/#id32>`_  guidelines closely follows PEP 0001.  \n",
    "    \n",
    ".. note ::\n",
    "\n",
    "    Most Active LPEPs include a Next Action Section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPEP 001: Implementing Coding and Package Standards\n",
    "\n",
    "- **Status: Active**\n",
    "- **Type: Standards Track**\n",
    "- **Date: Epoch**\n",
    "- **Current Version: 0.1**\n",
    "\n",
    "### Standards\n",
    "\n",
    "This LPEP preserves best practices, standards or customs for develpopers that maintain code consistency.  Tne following micro-PEPs are numerically assigned.  New micro-PEPs will be added over time or modified with caution.\n",
    "\n",
    "1. A *General Convention* will be standardized for internal code, such that the inner layer(s) is/are consistently returned as a list of floats i.e. `400.0-[200.0]-800.0` and `400.0-[100.0-100.0]-800.0`.  This format is used to maintain type checking consistency within the code.  External use by the user input is not bound by this restriction however; shorthand notation is fine too, e.g. `400-200-800`.  Such notation will be internally converted to the General Convention.\n",
    "2. Except for user values such as layer thicknesses and total calculations (microns, um), all other internal, dimensional variables will assume SI units (i.e. meters, m).  These values will be converted for convenience for the user in the DataFrames, (e.g. millimeters, mm).  This PEP is adopted to limit excessive unit conversions within code.\n",
    "3. Per PEP 8, semi-private variables are marked with a single preceding underscore, i.e. `_check_layer_order()`.  This style is used to visually indicate internal methods/attributes, not particularly important for the user. Double underscores will only be used (sparingly) to prevent name collisions.  Internal hook methods with use both trailing and leading underscores, e.g. `_use_model_`.\n",
    "4. The true lamina thickness value (`t_`) will remain constant in the DataFrame and not vary with height (`d_`).\n",
    "5. In general, use convenient naming conventions that indicate modules where the objects originates, e.g. `FeatureInput` object. However, whenever possible, aim to use descriptive names that reduce confusion over convienient names, e.g. `LaminateModel` object instead of `ConstructsTheories` object.\n",
    "6. For compatibilty checks, run nose 2.x and nose 3.x before commits to target Py3to2 errors in tests, (e.g. `dict.values()`).\n",
    "7. Materials parameters are handled internally as a dict formatted in *Standard Form* (compatible with pandas DataFrames) , but it is displayed as a DataFrame when the materials attribute is called by the user.  The Standard form comprises a dict of materials property dicts. By contrast, a *Quick Form* is allowed as input by the user, but interally converted to the Standard Form.  \n",
    "    - Quick Form:\n",
    "    `{Material: [Modulus value, Poissons value], ...}`\n",
    "    - Standard Form: \n",
    "    `{'Modulus': {'Mat1': value,...},'Poissons': {'Mat1': value, ...}`\n",
    "8. Internally, middle layers from `Geometry` return the full thickness, not the symmetric thickness.\n",
    "9. Thicknesses will be handled this way.  \n",
    "    - $t$ is the total laminate thickness\n",
    "    - $t_k$ is the thickess at lamina `k`\n",
    "    - `t_` is the internal variable that refers to true lamina thicknesses.\n",
    "    - The DataFrame column label $t(um)$ will refer to lamina thicknesses.\n",
    "    - `h_` is also a lamina thickness, relative to the neutral axis; therefore middle layers (and `h_`) are symmeric about the neutral axis $t_{middle} = 2h_{middle}$\n",
    "10. p=2 give the most critical points to calculate - interfacial minima and maxima per layer.  Maxima correlate with the 'interface' `label_` and minima correspond to the 'discont.' `label_`.  However, at minimun it is importannt to test with p>=5 to calculate all point types (interfacial, internals and neutural axes) perferably for odd plies.\n",
    "11. in geometry strings, the dash character `-` separates layer types outer-inner-middle.  The comma `,` separates other things, such as similar layer types, such as inner_i -[200,100,300]-.  The following is an invalid geomtry string `'400-[200-100-300]-800'`.\n",
    "12. Two main branches will be maintained: \"master\" and \"stable\". \"master\" will reflect development versions, always ahead of stable releases.  \"stable\" will remain relatively unchanged except for minor point releases to fix bugs. \n",
    "13. This package will adopt [semantic versioning](http://semver.org/) format (MAJOR.MINOR.PATCH).\n",
    "    >- MAJOR version when you make incompatible API changes,\n",
    "    >- MINOR version when you add functionality in a backwards-compatible manner, and\n",
    "    >- PATCH version when you make backwards-compatible bug fixes.\n",
    "14. Package releases pin dependencies to prevent breakage due to dependency patches/updates.  This approach assumes the development versions will actively address patches to latest denpendency updates prior to release.  User must be aware that installing older versions may downgradetheir current installs.\n",
    "15. Use incremented, informative names for tests, e.g. the following says  \"testing a Case method called \"plot\" with x feature:\n",
    "    - `test_<class>_mtd_<method name>_<optional feature>#` \n",
    "    - `test_<class>_prop_<property name>_<optional feature>#`.  \n",
    "    \n",
    "    Class tests are ordered as below:\n",
    "        - <class> Args: args\n",
    "        - <class> Keywords: kw\n",
    "        - <class> Attribtutes: attr\n",
    "        - <class> Special Methods: spmthd\n",
    "        - <class> Methods: mthd\n",
    "        - <class> Properties: prop\n",
    "    \n",
    "    Function tests apply similarly, where appropriate.  Features are appended and purpose:\n",
    "        - `test_<func>_<feature 1>_<feature ...>_<purpose>#`\n",
    "\n",
    "\n",
    "### Copyright \n",
    "\n",
    "This document has been placed in the public domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPEP 002:  Extending `Cases` with Patterns\n",
    "\n",
    "\n",
    "- **Status: Deferred**\n",
    "- **Type: Process**\n",
    "- **Date: October 01, 2015**\n",
    "- **Current Version: 0.4.4b**\n",
    "\n",
    "\n",
    "### Motivation\n",
    "\n",
    "As of 0.4.4b, a `Cases` object supports a group of cases distinguished by  different ps where each case is a set of LaminateModels with some pattern that relates them.  For example, an interesting plot might show multiple geometries of:\n",
    "\n",
    "- Pattern A: constant total thickness\n",
    "- Pattern B: constant midddle thickness\n",
    "\n",
    "In this example, two cases are represented, each comprising LaminateModels with geometries satisfying a specific pattern.  Currently `Cases` does not support groups of cases distinguished by pattern, but refactoring it thusly should be simple and will be discussed here.  Our goal is to extend the `Cases` class to generate cases that differ by parameters other than `p`.\n",
    "\n",
    "\n",
    "### Desired Ouptut\n",
    "\n",
    "To plot both patterns together, we need to feed each case seperately to plotting functons.  We need to think of what may differ between cases:\n",
    "\n",
    "- p\n",
    "- loading parameters\n",
    "- material properties\n",
    "- different geometries, similar plies \n",
    "- number plies (complex to plot simulataneously)\n",
    "- orientation (not implemented yet)\n",
    "- ...\n",
    "\n",
    "Given the present conditions, the most simple pattern is determined by geometry.  Here are examples of cases to plot with particular patterns of interest.\n",
    "\n",
    "```python\n",
    "# Pattern A: Constant Total Thickness\n",
    "case1.LMs = [<LamAna LaminateModel object (400-200-800) p=5>,\n",
    "             <LamAna LaminateModel object (350-400-500) p=5>,\n",
    "             <LamAna LaminateModel object (200-100-1400) p=5>,\n",
    "            ]\n",
    "\n",
    "# Pattern B: Constant Middle and Total Thickness\n",
    "case2.LMs = [<LamAna LaminateModel object (400-200-800) p=5>,\n",
    "             <LamAna LaminateModel object (300-300-800) p=5>,\n",
    "             <LamAna LaminateModel object (200-400-800) p=5>,\n",
    "            ]\n",
    "```\n",
    "\n",
    "### Specification\n",
    "\n",
    "To encapsulate these patterns, we can manually create a dict of keys and case values.  Here the keys label each case by the pattern name, which aids in tracking what the cases do.  The `Cases` dict should emulate this modification to support labeling.\n",
    "\n",
    "```python\n",
    "cases = {'t_total': case1,\n",
    "         'mid&t_total': case2,}\n",
    "```\n",
    "\n",
    "`Cases` would first have to support building different cases given groups of different geometry strings.  Perhaps given a dict of geometry strings, the latter object gets automatically created. For example, \n",
    "\n",
    "```python\n",
    "patterns = {\n",
    "    't_total': ['400-200-800', '350-400-500', '200-100-1400'],\n",
    "    'mid&t_total': ['400-200-800', '300-300-800', '200-400-800'],\n",
    "}\n",
    "```\n",
    "\n",
    "The question then would be, how to label different ps or combine patterns i.e., t_total and ps.  Advanced `Cases` creation is a project for another time.  Meanwhile, this idea of plotting by dicts of this manner will be beta tested. \n",
    "\n",
    "\n",
    "### Next Actions\n",
    "\n",
    "- Objective: organize patterns of interest and plot them easily with `Case` and `Cases` plot methods.\n",
    "    - Refactor Case and Cases to handle dicts in for the first arg.\n",
    "    - Parse keys to serve as label names (priority). \n",
    "    - Iterate the dict items to detect groups by the comma and generate a caselets for cases, which get plotted as subplots using an instanace of `output_.PanelPlot'\n",
    "\n",
    "\n",
    "### See Also\n",
    "\n",
    "- LPEP 003\n",
    "\n",
    "\n",
    "### Copyright \n",
    "\n",
    "This document has been placed in the public domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPEP 003:  A humble case for caselets\n",
    "\n",
    "- **Status: Replaced**\n",
    "- **Type: Process**\n",
    "- **Date: October 05, 2015, March 15, 2016**\n",
    "- **Current Version: 0.4.4b, 0.4.11**\n",
    "\n",
    "\n",
    "### Motivation\n",
    "\n",
    "By the final implementation of 0.4.4b, each case will generate a plot based on laminate data given loading, material and geometric information. Single plots are created, but subplots are desired also, where data can be compared from  different cases in a single figure.  This proposal suggests methods for organizing such plotting data by defining a new case-related term, a `caselet` object and its application to a figure object comprising subplots, based on a ~~`PanelPlot`~~ `FigurePlot` subclass.\n",
    "\n",
    "\n",
    "### Definitions\n",
    "\n",
    "- **LaminateModel** (LM): an object that combines physical laminate dimensions and laminate theory data, currently in the form of DataFrames.\n",
    "- **case**: a group of LMs; an analytical unit typically sharing similar loading, material and geometric parameters.  The final outcome is commonly represented by a matplotlib axes.\n",
    "- **cases**: a group of cases each differentiated by some \"pattern\" of interest, e.g. p, geometries, etc. (see LPEP 002).\n",
    "- **caselet**: (new) ~~a sub-unit of a case or cases object.  Forms are either a single geometry string, list of geometry strings or list of cases.~~ The final outcome is strongly associated with data pertaining to a matplotlib axes, or subplot component (not an instance or class). (See LPEP 006 for revised definitions)\n",
    "- **input**: (new) The user arg passed to `Case()` or `Cases()`.\n",
    "\n",
    "\n",
    "### Types of Inputs\n",
    "\n",
    "The generation of caselet plots as matplotlib subplots requires us to pass objects into `Case(*input*)` or `Cases(*input*)`. To pass in caselet data, the *input* must be a container (e.g. list, tuple, dict, etc.) to encapsulate the objects.  The container of any type contain caselets or various types including a string, list or case.  \n",
    "\n",
    "For example, if a list is used, there are at least three options for containing caselets:\n",
    "\n",
    "1. A list of geometry strings: `type(caselet) == str`\n",
    "1. A nested list of geometry strings: `type(caselet) == list`\n",
    "1. A list of cases: `type(caselet) == <LamAna.distributions.Case object>`\n",
    "\n",
    "If a dict is used to contain caselets, the latter options can substitute as dict values.  The keys can be either integers or explict labels.  \n",
    "\n",
    "*NOTE: as of 0.4.5, the List will be the default input type of caselets .  The dict may or may not be implemented in future versions.*\n",
    "\n",
    "---\n",
    "\n",
    "~~The following is completed implementation as of v0.4.5.~~\n",
    "\n",
    "\n",
    "\n",
    "### Forms of Caselet Inputs\n",
    "\n",
    "- Container : list or dict\n",
    "    Contains the various types that represent cases\n",
    "\n",
    "- Contained : str, list or str, cases (0.4.11.dev0)\n",
    "    Input types that represent, user-defined separate cases.\n",
    "\n",
    "\n",
    "#### List of Caselets\n",
    "\n",
    "Here we assume the input container type is a homogenous list of caselets.  The caselets can be  either geometry strings, lists of geometry strings or cases.\n",
    "\n",
    "\n",
    "##### Caselets as geometry strings\n",
    "\n",
    "(Implemented) The idea behind caselets derives from situations where a user desires to produce a figure of subplots.  Each subplot might show a subset of the data involved.  The simplest situation is a figure of subplots where each subplot (a caselet) plots a different geometry.\n",
    "\n",
    "    >>> import LamAna as la\n",
    "    >>> from LamAna.models import Wilson_LT as wlt\n",
    "    >>> dft = wlt.Defaults()\n",
    "    >>> input = ['400-200-800', '350-400-500', '200-100-1400']\n",
    "    >>> case = la.distributions.Case(dft.load_params, dft.mat_props)\n",
    "    >>> case.apply(input)\n",
    "    \n",
    "    Figure of three subplots with different geoemetries. \n",
    "    \n",
    "    .. plot::\n",
    "            :context: close-figs\n",
    "    \n",
    "            >>> case.plot(separate=True)\n",
    "\n",
    "Here the `Case.plot()` method plots each geometry independently in a grid of subplots using a special`separate` keyword.  *NOTE: Currently this feature uses `_multiplot()` to plot multiple subplots.  Future implentation should include `Panelplot`*  The `Cases` class is a more generic way to plot multiple subplots, which does not require a `separate` keyword and handles other caselet types.\n",
    "\n",
    "    >>> cases = la.distributions.Cases(input)\n",
    "    \n",
    "    Figure of three subplots with different geoemetries. \n",
    "    \n",
    "    .. plot::\n",
    "            :context: close-figs\n",
    "    \n",
    "            >>> cases.plot()\n",
    "\n",
    "*Caselets as lists*\n",
    "\n",
    "(Implemented) Another example, if we deisre to build a figure of subplots where each subplot is a subset of a case showing constant total thickness, constant middle thickness, constant outer thickness. We define each subset as a `caselet` and could plot them each scenario as follows:\n",
    "\n",
    "    >>> import LamAna as la\n",
    "    >>> list_patterns = [\n",
    "            ['400-200-800', '350-400-500', '200-100-1400'],\n",
    "            ['400-200-800', '300-300-800', '200-400-800'],\n",
    "            ['400-200-800', '400-100-1000', '400-300-600']\n",
    "        ]\n",
    "    >>> cases = la.distributions.Cases(list_patterns)\n",
    "    \n",
    "    Figure of three subplots with constant total thickness, middle and outer. \n",
    "    \n",
    "    .. plot::\n",
    "            :context: close-figs\n",
    "    \n",
    "            >>> cases.plot()\n",
    "\n",
    "*Caselets as cases*\n",
    "\n",
    "(Implemented) What if we already have cases?  Here is a means of comparing different cases on the same figure.\n",
    "\n",
    "    >>> import LamAna as la\n",
    "    >>> list_caselets = [\n",
    "            ['400-200-800'], \n",
    "            ['400-200-800', '400-400-400'],\n",
    "            ['400-200-800', '400-400-400', '350-400-500']\n",
    "        ]\n",
    "    >>> case1 = la.distributions.Case(dft.load_params, dft.mat_props)\n",
    "    >>> case2 = la.distributions.Case(dft.load_params, dft.mat_props)\n",
    "    >>> case3 = la.distributions.Case(dft.load_params, dft.mat_props)\n",
    "    >>> case1.apply(list_caselets[0])\n",
    "    >>> case2.apply(list_caselets[1])\n",
    "    >>> case3.apply(list_caselets[2])\n",
    "    \n",
    "    >>> list_cases = [case1, case2, case3]\n",
    "    >>> cases = la.distributions.Cases(list_patterns)\n",
    "    \n",
    "    Figure of three subplots with constant total thickness and different geometries. \n",
    "    \n",
    "    .. plot::\n",
    "            :context: close-figs\n",
    "    \n",
    "            >>> cases.plot()\n",
    "\n",
    "---\n",
    "The following will not be implemented in v0.4.5.\n",
    "\n",
    "#### Dict of Caselets\n",
    "\n",
    "*Key-value pairs as labeled cases.*\n",
    "\n",
    "(NotImplemented) What if we want to compare different cases in a single figure?  We can arrange data for each case per subplot.  We can abstract the code of such plots into a new class `PanelPlot`, which handles displaying subplots.  Let's extend `Cases` to make a `PanelPlot` by supplying a dict of cases.\n",
    "\n",
    "    >>> dict_patterns = {'HA/PSu': case1,\n",
    "    ...                  'mat_X/Y': case2,}\n",
    "    >>> cases = la.distributions.Cases(dict_patterns)\n",
    "\n",
    "    Figure of two subplots with three differnt patterns for two laminates with different materials. \n",
    "    \n",
    "    .. plot::\n",
    "            :context: close-figs\n",
    "    \n",
    "            >>> cases.plot()\n",
    "\n",
    "*Key-value pairs as labeled lists*\n",
    "\n",
    "(NotImplemented) We could explicitly try applying a dict of patterns instead of a list.  This inital labeling by keys can help order patterns as well as feed matplotlib for rough plotting titles.  Let's say we have a new case of different materials.\n",
    "\n",
    "    >>> dict_patterns = {\n",
    "    ...    't_tot': ['400-200-800', '350-400-500', '200-100-1400'],\n",
    "    ...    't&mid': ['400-200-800', '300-300-800', '200-400-800'],\n",
    "    ...    't&out': ['400-200-800', '400-100-1000', '400-300-600']\n",
    "    ... }\n",
    "    >>> new_matls = {'mat_X': [6e9, 0.30],\n",
    "    ...              'mat_Y': [20e9, 0.45]}\n",
    "    >>> cases = la.distributions.Cases(\n",
    "    ...     dict_patterns, dft.load_params, new_matls\n",
    "    ... )\n",
    "\n",
    "    Figure of three subplots with constant total thickness, middle and outer for different materials. \n",
    "    \n",
    "    .. plot::\n",
    "            :context: close-figs\n",
    "    \n",
    "            >>> cases.plot()\n",
    "\n",
    "*Key-value pairs as numbered lists*\n",
    "\n",
    "(NotImplemented) We can make a caselets in dict form where each key enumerates a list of geometry strings.  This idiom is probably the most generic.  ~~This idiom is currently accepted in `Cases.plot()`.~~  Other idioms may be developed and implemented in future versions. \n",
    "\n",
    "    >>> dict_caselets = {0: ['350-400-500',  '400-200-800', '200-200-1200',\n",
    "    ...                      '200-100-1400', '100-100-1600', '100-200-1400',]\n",
    "    ...                  1: ['400-550-100', '400-500-200', '400-450-300',\n",
    "    ...                      '400-400-400', '400-350-500', '400-300-600'],\n",
    "    ...                  2: ['400-400-400', '350-400-500', '300-400-600',\n",
    "    ...                      '200-400-700', '200-400-800', '150-400-990'],\n",
    "    ...                  3: ['100-700-400', '150-650-400', '200-600-400',\n",
    "    ...                      '250-550-400', '300-400-500', '350-450-400'], \n",
    "    ...                 }\n",
    "    >>> #dict_patterns == dict_caselets\n",
    "    >>> cases = la.distributions.Cases(dict_caselets)\n",
    "\n",
    "    Figure of four subplots with different caselets.  Here each caselet represents a different case (not always the situation). \n",
    "    \n",
    "    .. plot::\n",
    "            :context: close-figs\n",
    "    \n",
    "            >>> cases.plot()\n",
    "\n",
    "\n",
    "### Specification\n",
    "\n",
    "Currently, the specification outlined here is to convert a caselet input into a caselet using a conversion function.  Implementation of a formal caselet object are subject to future consideration.\n",
    "\n",
    "The current application is to feed a `Cases.plot()` method  with input which is converted to one of the latter types of caselets.  At the moment, type handling for caselets occurs in `Cases()`.  This section proposes that type handling for caselets be implemented in the `input_` module instead for general use.\n",
    "\n",
    "This function will handle processing of various input container types.\n",
    "\n",
    "\n",
    "```python\n",
    "def to_caselet(input):\n",
    "    '''Return a Case obect given an input.\n",
    "    \n",
    "    This function accepts each item of a container and processes them into a Case.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input : str, list (of str), case\n",
    "        This user input becomes a Case object, representing a caselet - a subcomponent\n",
    "        of other related cases.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Uses error handling to convert an input into one of the defined caselet types\n",
    "    str, list of str or case (see LPEP 003).  These caselets derive from homogenous types.\n",
    "    \n",
    "    Heterogenous caselets are not handled, but may be implemented in the future.\n",
    "       \n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    FormatError\n",
    "        Only a geometry string, homogenous list of geometry strings or case is accepted.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Case object\n",
    "        Integer-case, key-value pairs.  \n",
    "    \n",
    "    '''\n",
    "    try:\n",
    "        # Assuming a list of geometry strings\n",
    "        case_ = la.distributions.Case(self.load_params, self.mat_props)\n",
    "        if unique:\n",
    "            case_.apply(input, unique=True)\n",
    "        else:\n",
    "            case_.apply(input)\n",
    "        self.caselets = [case_]\n",
    "        # TODO: Brittle; need more robust try-except\n",
    "    except(AttributeError, TypeError):             # raised from Geometry._to_gen_convention()\n",
    "        try:\n",
    "            # If a list of lists\n",
    "            flattened_list = list(it.chain(*caselets))\n",
    "            # lists are needed for Cases to recognize separate caselets\n",
    "            # automatically makes a unique set\n",
    "            #print(caselets)\n",
    "            # TODO: what else is _get_unique doing?\n",
    "            ##self.caselets = [self._get_unique(flattened_list)]\n",
    "            #print(self.caselets)\n",
    "        except(TypeError):\n",
    "            # if a list of cases, extract LMs, else raise\n",
    "            flattened_list = [LM.Geometry.string for caselet in caselets\n",
    "                              for LM in caselet.LMs]\n",
    "            # list is needed for Cases to recognize as one caselet\n",
    "            # automatically makes a unique set\n",
    "            ##self.caselets = [self._get_unique(flattened_list)]\n",
    "            #print(self.caselets)\n",
    "     raise FormatError('Caselet type is not accepted.  Must be str, list of strings or case') #?\n",
    "     \n",
    "\n",
    "```\n",
    "\n",
    "    '''\n",
    "    Need to iterate caselets (lists of objects) to package the order of the data.\n",
    "    Then pass that data into the plot functions.  Plot functions should simply\n",
    "    make an axes for each data unit, then return an ax (for singleplot) or figure\n",
    "    (for multiplot).\n",
    "\n",
    "    1. Case only need a single list of input because it only handles one case/time.\n",
    "    2. Cases takes multiple lists or case objects\n",
    "       - may require separating a caselet into cases bases on what's given.\n",
    "\n",
    "    A Caselets object should accept either number or inputs.  Should rearrange caselets.\n",
    "    Should return a rearrange caselet input.  If this self is passed in, the order\n",
    "    of cases should be preserved\n",
    "\n",
    "    '''\n",
    "\n",
    "### Next Actions\n",
    "\n",
    "- Objective: Make abstract `PanelPlot` class that accepts dicts of LMs for cases to output figures of caselets or cases. \n",
    "    - build `PanelPlot` which wraps matplotlib subplots method.\n",
    "    - inherit from `PanelPlot` in `Case.plot()` or `Cases.plot()`\n",
    "    - implement in `output_`\n",
    "    - make plots comparing different conditions in the same `Case` (caselets)\n",
    "    - ~~make plots comparing different cases using `Cases`~~\n",
    "- Abstract idiom for building caselets accepted in `Cases.plot()`.\n",
    "- Implement general caselet converter, error-handler in `input_`\n",
    "- Make a caselets class.\n",
    "- Revise LPEP to accept LM or LMs as caselet types; refactor `to_caselet` to handle these types.  See `output_._multiplot`, which defines caselet differently.  \n",
    "\n",
    "### See Also\n",
    "\n",
    "- LPEP 002\n",
    "\n",
    "### Copyright \n",
    "\n",
    "This document has been placed in the public domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPEP 004:  Refactoring class `Stack`\n",
    "\n",
    "- **Status: Draft**\n",
    "- **Type: Process**\n",
    "- **Date: October 20, 2015, March 17, 2016 (revised)**\n",
    "- **Current Version: 0.4.4b1, 0.4.11**\n",
    "\n",
    "\n",
    "### Motivation\n",
    "    \n",
    "Inspired to adhere to classic data structures, we attempt to refactor some classes.  The present `la.constructs.Stack` class is not a true stack.  Athough built in a LIFO style, there are no methods for reversing the stack. It may be beneficial to the user to add or delete layers on the fly.  Stacks, queues and other data structures have methods for such manipulations.  Here are some ideas that entertain this train of thought.\n",
    "\n",
    "### Desired Output\n",
    "\n",
    "- Insert and remove any layers\n",
    "- Access geometry positions in an index way\n",
    "\n",
    "\n",
    "### Specification\n",
    "\n",
    "- Make stacks from deques\n",
    "- Extend Stack to interpret from geometry strings also\n",
    "\n",
    "\n",
    "### Examples\n",
    "    \n",
    "```python\n",
    ">>> LM = la.distributions.Cases('400-200-800').LMs\n",
    ">>> LM.insert('[:,100]')          # eqv. ':-[:,100]-:' \n",
    ">>> print(LM.geometry, LM.nplies)\n",
    "<Geometry object (400-[200,100]-800)>, 7\n",
    "\n",
    ">>> LM.remove('middle')\n",
    ">>> print(LM.geometry, LM.nplies)\n",
    "<Geometry object (400-[200,100]-0)>, 6\n",
    "\n",
    ">>> LM.remove(['outer', 'inner'])\n",
    "StackError 'If inner layers are removed, outer layers must exist.' \n",
    "\n",
    "```\n",
    "\n",
    "### Next Actions\n",
    "\n",
    "- Write specification for using deques\n",
    "- Write specification for implementing geo_string interpretation.\n",
    "- Develop the idea of duples in tandem\n",
    "\n",
    "\n",
    "### See Also\n",
    "\n",
    "- `analyze_geostrings()`: interpret strings nplies, thickness, order.\n",
    "\n",
    "\n",
    "### Copyright \n",
    "\n",
    "This document has been placed in the public domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPEP 005: Making Concurrent `LaminateModels` with the new `asyncio`\n",
    "\n",
    "- **Status: Draft**\n",
    "- **Type: Process**\n",
    "- **Date: February 23, 2016**\n",
    "- **Current Version: 0.4.10**\n",
    "\n",
    "\n",
    "### Motivation\n",
    "    \n",
    "The idea of concurrency offers a potential option for improving creation of LamAna objects.  For instance, if 10 `LaminateModels` are called to be made, rather then waiting for each object to instantiate serially, it may be better to create them in parallel. This proposal is entertains current object creation using concurrency, and it is adapted from this [simple, well written set of examples](https://pymotw.com/3/asyncio/coroutines.html) of coroutines and chained coroutines.\n",
    "\n",
    "\n",
    "### Definitions\n",
    "\n",
    "- G : Geometry object\n",
    "- FI : FeatureInput object\n",
    "- St : Stack\n",
    "- Sp : Snapshot\n",
    "- L : Laminate\n",
    "- LM : LaminateModel\n",
    "\n",
    "When `la.distributions.Case.apply()` is called, the `get_LaminateModel()` function creates a generated list of LaminateModels.  A series of objects a created accessing 3 core modules.  \n",
    "\n",
    "$$ \\big[G_{input\\_} \\rightarrow FI_{distributions-input\\_}\\big] \\longrightarrow \\big[St \\rightarrow Sp \\rightarrow L \\rightarrow LM \\big]_{constructs} $$\n",
    " \n",
    "When `apply()` is called, it has to **wait** for other serial processes to finish in a certain **order** before completing.  These characteristics of waiting on ordered processes **may** qualify the LamAna architecture as a candidate for concurrency features in the new Python 3.5 `asyncio` module.  \n",
    "\n",
    "\n",
    "### Implemented Chained Coroutines\n",
    "\n",
    "We attempt to apply these concepts to LamAna.  A summary of the main coroutine is outlined below.\n",
    "\n",
    "```python\n",
    "\n",
    "import asyncio\n",
    "\n",
    "async def get_LaminateModel(geo_string):\n",
    "    '''Run set of processes in order to give finally create a LaminateModel from a geo_string.'''\n",
    "    # conv_geometry converts a geo_string to general convention \n",
    "    # TODO: include geo_string caching\n",
    "    \n",
    "    # TODO: comvert these objects to coroutines (or keep as generators?)\n",
    "    G = await = la.input_.Geometry(conv_geomtry)\n",
    "    FI = await la.input_.BaseDefaults.get_FeatureInput(G, **kwargs)      # rewrite FeatureInput\n",
    "    St = await la.constructs.Stack(FI)\n",
    "    Sp = await la.constructs.Snapshot(St)                                # snapshot   \n",
    "    L = await la.constructs.Laminate(Sp)                                 # LFrame   \n",
    "    LM = await la.constructs.Laminate(L)                                 # LMFrame\n",
    "    \n",
    "# The main event loop\n",
    "event_loop = asyncio.get_event_loop()\n",
    "for geo_string in geo_strings:                                           # unsure if this would work\n",
    "    try:\n",
    "        # Consider alternatives to this default loop\n",
    "        laminate_model = event_loop.run_until_complete(get_LaminateModel(geo_string))\n",
    "    finally:\n",
    "        event_loop.close()\n",
    "\n",
    "```\n",
    "\n",
    "*NOTE: It is  unclear how to advance the geo_strings iterable object in the default asyncio loops*\n",
    "\n",
    "\n",
    "### Vetting\n",
    "\n",
    "Pros:\n",
    "\n",
    "- Possbible concurrency, multitasking of LaminateModel creation\n",
    "- Clear, explicit illustration of order\n",
    "\n",
    "Cons:\n",
    "\n",
    "- Limited to Python 3.5\n",
    "\n",
    "### Next Actions\n",
    "\n",
    "- Look into advancing iterables in an asynio default loop\n",
    "- Use mock objects to test this LPEP as proof of concept\n",
    "\n",
    "### Copyright \n",
    "\n",
    "This document has been placed in the public domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPEP 006:  Defining LamAna Objects\n",
    "\n",
    "\n",
    "- **Status: Draft** \n",
    "- **Type: Informational**\n",
    "- **Date: March 17, 2016**\n",
    "- **Current Version: 0.4.11**\n",
    "\n",
    "\n",
    "### Motivation\n",
    "\n",
    "This LPEP is written to clarify certain types used within LamAna documentation and codebase.\n",
    "\n",
    "### Definitions\n",
    "\n",
    "#### Laminate classifications\n",
    "\n",
    "- **Symmetric**: a laminate with symmetry across the neutral axis.\n",
    "- **Asymmetric**: non-symmetry across the netural axis. \n",
    "\n",
    "#### Representations\n",
    "\n",
    "- **geo_string** (g): a geometry string typically formatted to general convention (see LPEP 001)\n",
    "- **Geo_object** (G): a `Geometry` object, an instance of the Geometry class\n",
    "- **Geo_orient** (GO) (NotImplemented): a `GeoOrient` object, containing in-plane, directional, ply-angle information\n",
    "\n",
    "#### Layer types (lytpe)\n",
    "\n",
    "- **outer**: the top and bottom-most layers\n",
    "- **inner_i**: a list (or string representation) of all inner layer thicknesses; inners refers to a subset of inner_i \n",
    "- **inner**: an internal, non-middle, non-outer layer\n",
    "- **middle**: for odd plies, the center layer;  for symmetric laminates, this layers passing through the neutral axis\n",
    "\n",
    "#### Geometry string containers\n",
    "\n",
    "Pythonic objects used to signify groups of layer thickness:\n",
    "\n",
    "- **list**: a pythonic list of inner layers, e.g. [100, 100, 50].  Each entry represents equivalent layer thicknesses for both tensile and compressive sides.\n",
    "\n",
    "- **token**: pertaining to one of the layer types\n",
    "\n",
    "- **duple** (NotImplemented): a tuple of dual layer thicknesses for corresponding (tensile, compressive) layers, e.g. (100,300).  Each entry represents a significant thickness of a tensile/compressive side for a given layer type. Zero is also not allowed (0,400).  A duple replaces one of the thickness positions in a geometry string.  The sum of a duple contributes to the total laminate thickness.  By definition, duples are only used to specify asymmetric geometries, therefore repeated values are disallowed e.g. (400,400).  Also, since middles are singular constructs, duples are disallowed for middle layers.\n",
    "\n",
    "#### Geometry strings\n",
    "\n",
    "**Regular geometry strings**: a simple, *symmetric* stacking sequence of outer, inner_i and middle layers. e.g.  \n",
    "```python\n",
    "- '400-[200]-800'                                          # simple\n",
    "- '400-[150,50]-800'                                       # inner_i\n",
    "```  \n",
    "These strings follow a simple algorithms for calculating layer thicknesses:\n",
    "\n",
    "$$ t_{total, outer} = 2t_{outer} $$\n",
    "$$ t_{total, inner} = 2t_{inner_i} $$\n",
    "$$ t_{total, inner_{i}} = 2\\sum_{i}^m t_{inner}$$  \n",
    "$$ t_{total} = 2(t_{outer} + t_{inner_i}) + t_{middle} $$\n",
    "$$ n_{plies} = 2(n_{outer} + n_{inner_i}) + n_{middle} $$\n",
    "\n",
    "**Irregular geometry strings**: includes *assymmetric* laminates; involves\n",
    "```python\n",
    "- '(300,100)-[150,50]-800'                                 # outer duple\n",
    "- '400-[150,(75,50),25]-800'                               # inner duple\n",
    "- '(300,100)-[150,(75,50),25]-800'                         # outer and inner duple\n",
    "```\n",
    "These strings can follow more complex algorithms for calculating layer thickness.  For every $ith$ item in the list of inner_i and $jth$ index within an $i$ (duple or non), where $m$ is the end of the squence and $C=1$ for duples and $C=2$ for non-duples:\n",
    "\n",
    "$$ t_{total, outer} = C\\sum_{i}^m\\sum_{j}^{m=2} t_{outer} $$\n",
    "$$ t_{total, inner} = C\\sum_{j}^{m_j} t_{inner} $$\n",
    "$$ t_{total, inner_i} = C\\sum_{i}^m\\sum_{j}^{m_j} t_{inner} $$ \n",
    "$$ t_{total} = t_{outer} + t_{inner_i} + t_{middle} $$\n",
    "$$ n_{plies} = C_1\\sum_{i}^{m_i}\\sum_{j}^{m_j} n_{outer} + C_2\\sum_{i}^{m_i}\\sum_{j}^{m_j} n_{inner} + n_{middle} $$\n",
    "\n",
    "\n",
    "#### Data structures \n",
    "\n",
    "Conceptual structures used to represent groups of data:\n",
    "\n",
    "- **packet**: a user-defined input data, e.g. a list of geometry strings.  This group are structured according to some pattern of interest (see LPEP 002).  A packet may become processed datasets (LMs) encased within a `Case` object.\n",
    "- **packets**: a group of packets, units that represent separated cases, e.g. a list of lists comprising geometry strings.  These groups are ordered according to desired output.\n",
    "- **stack**: the bottom-to-top (tensile-to-compressive) stacking sequence of laminated layers. Represented as lists, deques or other pertinent data structures. Regular stacks reverse inner_i order post middle layer.  Irregular stacks must parse duple indices, tensile and compressive. \n",
    "    - Regular stack: '400-[150,50]-800' --> [400.0, 150.0, 50.0, 800.0, 50.0, 150.0, 400.0]\n",
    "    - Irregular stack: '400-[(150,50)]-800' --> [400.0, 150.0, 800.0, 50.0, 400.0]\n",
    "- **LaminateModel** (LM): an object that combines physical laminate dimensions and laminate theory data, currently in the form of DataFrames.\n",
    "- **case**: an analytical unit typically sharing similar loading, material and geometric parameters. This object contains a group of LMs; the final outcome is commonly represented by a matplotlib axes.\n",
    "- **cases**: a group of cases each differentiated by some \"pattern\" of interest, e.g. p, geometries, etc. (see LPEP 002).  The final product is commonly represented as a matplotlib Figure.  Each group is a smaller case \"caselet\" \n",
    "- **caselet**: a samller case that is related to a larger group of cases.  This conceptual unit is finally manifested as a matplotlib subplot. \n",
    "\n",
    "#### Plotting objects\n",
    "\n",
    "- figureplot: a matplotlib Figure with attributes for quick access to plot objects. A base class for setting figure options and appearance, akin to a seaborn [FacetGrid](https://github.com/mwaskom/seaborn/blob/10bdb18f47bb5fc0a30d34954ff6f174b4cf5881/seaborn/axisgrid.py) \n",
    "- singleplot: a single matplotlib axes.\n",
    "- multiplot: a figure of singleplots represented in subplots.\n",
    "- feature plot: a LamAna plotting object based on a specific feature module e.g. `DistribPlot`\n",
    "\n",
    "\n",
    "### Examples\n",
    "\n",
    "```python\n",
    "Analyzed string Information\n",
    "\n",
    "Number of plies, total laminate thickness and stacking order \n",
    "(nplies, t_total, order)\n",
    "\n",
    "General Convention\n",
    "'400.0-[100.0,100.0]-800.0'\n",
    "# (7, 2.0, [400.0,100.0,100.0,800.0,100.0,100.0,400.0]) \n",
    "\n",
    "Duple\n",
    "'(300.0,100.0)-[(50.0, 150.0),100.0]-800.0\n",
    "# (7, 1.6, [300.0,50.0,100.0,800.0,100.0,150.0,100.0])\n",
    "\n",
    "```\n",
    "\n",
    "### Next Actions\n",
    "\n",
    "- Swap definitions of \"inner_i\" and \"inner\".\n",
    "\n",
    "\n",
    "### See Also\n",
    "\n",
    "- LPEP 001: General Convention\n",
    "\n",
    "\n",
    "### Copyright \n",
    "\n",
    "This document has been placed in the public domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPEP 007: Redesigning the `distributions` Plotting Architecture\n",
    "\n",
    "- **Status: Draft**\n",
    "- **Type: Process**\n",
    "- **Date: April 05, 2016**\n",
    "- **Current Version: 0.4.11**\n",
    "\n",
    "\n",
    "### Motivation\n",
    "\n",
    "The plotting functions were quickly put together prior to LamAna's offical release.  This original plotting architecture lacks robustness and scalability for future feature modules.  The current version of `Case.plot()` and `Cases.plot()` methods use non-public functions located the `output_` module for plotting single axes figures (\"single plots\") and multi-axes figures (\"multi plots\").  The purpose of this proposal is to lay out a robust, lucid architecture for plotting `distributions` and future feature module outputs.\n",
    "\n",
    "\n",
    "### Desired Ouptut\n",
    "\n",
    "...\n",
    "\n",
    "### Definitions\n",
    "\n",
    "See LPEP 007 for formal definitions.\n",
    "\n",
    "#### Basic Plot Options\n",
    "\n",
    "The following objects associate with lower level matplotlib objects:\n",
    "\n",
    "- singleplot, multiplot, figureplot\n",
    "\n",
    "The following definition pertains to a unique LamAna objects that inherits the latter objects:\n",
    "\n",
    "- DistribPlot: a class that handles the output of a `distributions` plot.  \n",
    "\n",
    "\n",
    "### Specification\n",
    "\n",
    "A `DistribPlot` should be given `LamainateModels`.  While iterating over `LaminateModels`, information is extracted (e.g. nplies, p) and axes are generated both combining plotting lines and separating unique laminates under various conditions.  THis class should inherit from a base that controls how a figure appears.  Through iterating the given argument, this class should determine whether the resulting figure should be a singleplot or multiplot.  Here is a sample signature for the `Distriplot`.  \n",
    "\n",
    "```python\n",
    "import lamana as la\n",
    "\n",
    "class _FigurePlot(object):\n",
    "    '''Return a matplotlib Figure with base control.'''\n",
    "    def __init__(self):\n",
    "        self.nrows = 1\n",
    "        self.ncols = 1\n",
    "\n",
    "        fig, ax = plt.subplots(self.nrows, self.ncols)\n",
    "        self = fig\n",
    "        self.axes = ax\n",
    "        self.naxes = len(ax)\n",
    "        self.x_data = fig.axes.Axes[0]\n",
    "        self.y_data = fig.axes.Axes[1]\n",
    "        #self.patches = extract_patches()\n",
    "\n",
    "    def update_figure():\n",
    "        '''Update figure dimensions.'''\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "\n",
    "class DistribPlot(_FigurePlot):\n",
    "    '''Return a distributions FigurePlot.\n",
    "\n",
    "    This class needs to process LaminateModels and honor the user-defined packages.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cases_ : Case or Cases object\n",
    "        The self object of the Case and Cases classes.  Relies on the pre-ordered\n",
    "        arrangement of the user-defined, package input.\n",
    "    kwargs : dict\n",
    "        Various plotting keywords.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    Entry points\n",
    "    - lamana.distributions.Case.apply: primarily singleplots unless separated\n",
    "    - lamana.distributions.Cases: primarily multiplots unless combined\n",
    "\n",
    "    '''\n",
    "    def __init__(self, cases_, **kwargs):\n",
    "        super(DistribPlot, self).__init__(cases_, **kwargs)\n",
    "        self = self.make_fig(cases_)\n",
    "        self.packages = cases_.packages                    # NotImplemented \n",
    "        \n",
    "\n",
    "    # Temporary\n",
    "    # TODO: these plotters need to be abstracted from distributions code.\n",
    "    def _singleplot(self, case):\n",
    "        singleplot = la.output_._distribplot(case.LMs)\n",
    "        return singleplot\n",
    "\n",
    "    def _multiplot(self, cases):\n",
    "        multiplot = la.output_._multiplot(cases)\n",
    "        return multiplot\n",
    "\n",
    "    def make_fig(self, cases_ordered):\n",
    "        '''Return a figure given cases data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cases_ordered : Case- or Cases-like\n",
    "            Contains data required to generate the plots.  Assumes the cases\n",
    "            preserve the the user-defined order at onset in the caselet_input.\n",
    "\n",
    "        '''\n",
    "        if isinstance(cases_ordered, la.distributions.Case):\n",
    "            # Give a single Case object\n",
    "            case = cases_ordered\n",
    "            fig = plt.figure()\n",
    "            ax = self._singleplot(case)\n",
    "            fig.axes.append(ax)\n",
    "        elif isinstance(cases_ordered, la.distributions.Cases:\n",
    "            # Give a Cases object\n",
    "            fig = self._multiplot(cases_ordered)\n",
    "            #plt.suptitle()\n",
    "            #plt.legend()\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                'Unknown distributions type was pass into {}.'.format(self.__class__)\n",
    "            )\n",
    "\n",
    "        return fig\n",
    "\n",
    "\n",
    "# Mock Implementations ---------------------\n",
    "\n",
    "# Handles singleplots from Case\n",
    "def Case.plot(self, **kwargs):\n",
    "    return la.output_.DistribPlot(*args, **kwargs)\n",
    "\n",
    "\n",
    "# Handles multiplots from Cases\n",
    "def Cases.plot(self, **kwargs):\n",
    "    return la.output_.DistribPlot(*args, **kwargs)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Examples\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "Singleplots\n",
    "\n",
    ">>> case = Case(['400-200-800', '400-400-400'])\n",
    ">>> singleplot = cases.plot()\n",
    "<matplotlib Figure>\n",
    ">>> multiplot.naxes\n",
    "1\n",
    "\n",
    "Multiplots\n",
    "\n",
    ">>> cases = Cases([['400-200-800', '400-400-400'], ['100-100-1600']])\n",
    ">>> multiplot = cases.plot()\n",
    "<matplotlib Figure>\n",
    ">>> multiplot.naxes\n",
    "2\n",
    "\n",
    ">>> multiplot.axes\n",
    "[<maplotlib AxesSupbplot>, <maplotlib AxesSupbplot>]\n",
    ">>> multiplot.packages                                     # orig. input\n",
    "[['400-200-800', '400-400-400'], ['100-100-1600']]\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Vetting\n",
    "\n",
    "\n",
    "\n",
    "### Next Actions\n",
    "\n",
    "- Add more attributes as ideas come about\n",
    "- Implement and test in future versions; revise `Case`, `Cases` and add `Packages`\n",
    "- \n",
    "\n",
    "\n",
    "### See Also\n",
    "\n",
    "- LPEP 002: on patterns\n",
    "- LPEP 003: packets; a revised form of caselets \n",
    "- LPEP 006: unoffical glossary\n",
    "- LPEP: 008 Formalizing Packets\n",
    "\n",
    "\n",
    "\n",
    "### Copyright \n",
    "\n",
    "This document has been placed in the public domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPEP 008: Formalizing `Packets`: input data for caselets\n",
    "\n",
    "\n",
    "- **Status: Draft**\n",
    "- **Type: Process**\n",
    "- **Date: April 07, 2016, (Rev. 04/10/16)**\n",
    "- **Current Version: 0.4.11**\n",
    "\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Multiplots require *structured data* to display plots correctly.  Ulitmately this product requires data that has be validated and organized in a simple and discernable format.  The `Packets` class is in `input_` datastructure that attempts to funnel various input type into a simple that that object after processing inputs as follows:\n",
    "\n",
    "- *validate* the geometry strings and input formats\n",
    "- *reformat* geometry data to according to internally-accepted, Generation Conventions.\n",
    "- *reorder* or rearrange data if exceptions are raised\n",
    "- *analyze* geometry data\n",
    "\n",
    "The user can now focus on arranging the data into analytical sub-groups (caselets).  This information new, restructured data is supplied to feature module objects such as `Case` or `Cases` that package the data according to the user-defined order.  Most importantly, plotting functions can simply iterate over the structured data an output plots that reflect this order.\n",
    "\n",
    "*NOTE: the `Packets` object was alpha coded in 0.4.11.dev0*\n",
    "\n",
    "\n",
    "### Desired Ouptut\n",
    "\n",
    "An enumerated dict of packet inputs.  \n",
    "\n",
    "This class should focus on cleaning and organizing the data for a feature module function.  *Let Case and Cases handle the data.*\n",
    "\n",
    "\n",
    "### Definitions/Keywords\n",
    "\n",
    "Terms such as *caselet* and *packet* have been developed during the planning phases of defining and refactoring `output_` objects into a logical, scaleable framework.  See [LPEP 006](#LPEP-006:--LamAna-Objects-1.6) for formal definitions.\n",
    "\n",
    "- packet, packets, LaminateModel, caselet, case, cases\n",
    "\n",
    "\n",
    "### Specification\n",
    "\n",
    "The simplest approach to ordering data is to handling all incoming inputs upfront.  The packets can them be funneled into a clean, restructured form.  As of 0.4.11, we introduced the `Packet` class, intended to convert packet inputs in said object.  \n",
    "\n",
    "Error handling is important for certain scenarios.  For example, given a list of geometry strings, a separate caselet must be  generated when: \n",
    "\n",
    "    1. The current nplies does not match the nplies in the current axes\n",
    "    2. Another set of ps is discovered\n",
    "\n",
    "As `Packets` must handle such an event by analyzing the raw geometry strings upfront.  Packet requirement may vary for different feature modules.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "class Packets(object):\n",
    "    '''Return a Packets object by processing user-defined packet inputs.\n",
    "\n",
    "    This class is an interface for converting various inputs to a formal datastructure.\n",
    "    It serves to precess inputs as follows:\n",
    "    - validate: geo_strings are a valid and interpretible\n",
    "    - reformat: convert a geo_string to General Convention\n",
    "    - reorder: split unequal-plied geo_strings in separate caselets (if needed)\n",
    "    - analyze: assessed for duples (NotImplemented)\n",
    "    \n",
    "    This class also handles unique exceptions to form new, separate packets based on various conditions:\n",
    "\n",
    "    1. The current nplies does not match the nplies in the current axes\n",
    "    2. Another set of ps is discovered\n",
    "    3. ...\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    packet : list\n",
    "        User input geometry strings; dictates how caselets are organized.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Fast, callable, ordered.  Contains int-caselet input, key-value pairs.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    - LPEP 003: original ideas on caselets\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Due to many container types, this class will be gradually extended:\n",
    "    - 0.4.12.dev0: supports the list container of str, lists of strs and cases.\n",
    "       \n",
    "    Examples\n",
    "    --------\n",
    "    See below.\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, packets):\n",
    "        self.packets = self.clean(packets)\n",
    "        self = to_dict(self.packets)\n",
    "        \n",
    "        self.nplies = None\n",
    "        self.t_total = None\n",
    "        self.size = len(self.packets)\n",
    "       \n",
    "    def clean(packets):\n",
    "        '''Return an analyzed reformatted, validated, orderd list of packets.\n",
    "        \n",
    "        Exploits the fine-grain iteration to extract geo_string data\n",
    "        via analyze_string().  \n",
    "        \n",
    "        '''\n",
    "        # Handle analyses and converting geo_strings to General Convention\n",
    "        \n",
    "        if self.nplies is None:\n",
    "            self.nplies = {}\n",
    "        if self.t_total is None:\n",
    "            self.t_total = {}\n",
    "\n",
    "        nplies_last = None\n",
    "        caselet_conv = []\n",
    "        \n",
    "        for packet in packets:\n",
    "            caselet_new = []\n",
    "            for geo_string in packet:                \n",
    "                # Validate\n",
    "                if is_valid(geo_string):\n",
    "                    # Reformat: Should raise error if invalid geo_string\n",
    "                    geo_string_conv = la.input_.to_gen_convention(geo_string)\n",
    "                    # Analyze: extract geo_string data while in the loop\n",
    "                    nplies, t_total, _ = la.input_.analyze_geostring(geo_string)\n",
    "                \n",
    "                # Store analyzed data in attributes\n",
    "                self.nplies.add(nplies)\n",
    "                self.t_total.add(t_total)\n",
    "\n",
    "                # Reorder: make new list for unequal nplies\n",
    "                if nplies != nplies_last:\n",
    "                    geo_string_conv = list(geo_string_conv)\n",
    "\n",
    "                caselet_new.append(geo_string_conv)\n",
    "                nplies_last = nplies\n",
    "                \n",
    "            # Ship for final handling and formatting \n",
    "            if len(caselet_new) == 1:\n",
    "                return self._handle_types(caselet_new)\n",
    "            else:\n",
    "                packets_conv.append(caselet_new)\n",
    "            return self._handle_types(packets_conv)\n",
    "   \n",
    "    def _handle_types(self):\n",
    "        '''Return the accepted packets format given several types.\n",
    "       \n",
    "        As of 0.4.11, the list is the only accpeted objecct container. At this\n",
    "        entry point, users should not be aware of Case or LaminateModels,\n",
    "        but they included anyway.\n",
    "       \n",
    "        '''\n",
    "        # Forward Compatibility ----------------------------------------- \n",
    "        # List of Case objects\n",
    "        # [case_a, case_b, ...] --> [['geo_str1', 'geo_str2'], ['geo_str1'], ...]\n",
    "\n",
    "\n",
    "        # A Single Case\n",
    "        # [case] or case --> ['geo_str1', 'geo_str2', 'geo_str3']       \n",
    "\n",
    "\n",
    "        # List of LaminateModels (LMs)\n",
    "        # [<LM1>, <LM2>, ...] --> [['geo_str1', 'geo_str2'], ['geo_str1'], ...]    \n",
    "\n",
    "       \n",
    "        # A Single LaminateModel (LM)\n",
    "        # [LM] or LM --> [['geo_str1', 'geo_str2', 'geo_str3'], ...]        \n",
    "       \n",
    "       \n",
    "        # -----------------------------------------------------------------\n",
    "        # List of lists or geo_strings\n",
    "        # [['geo_str1', 'geo_str2'], ['geo_str1'], ...] --> _\n",
    "       \n",
    "       \n",
    "        # List of geo_strings\n",
    "        # ['geo_str1', ...] --> _\n",
    "       \n",
    "       \n",
    "        # Single geo_string\n",
    "        # ['geo_str1'] or 'geo_str1' --> ['geo_str1']\n",
    "       \n",
    "       \n",
    "        except(AttributeError) as e:\n",
    "            raise FormatError(\n",
    "            'Caselet input () is an unrecognized format.'\n",
    "            ' Use a list of geo_strings'.format(e)\n",
    "            )\n",
    "       \n",
    "        pass      \n",
    "       \n",
    "    def to_dict(self)\n",
    "        '''Return an enumerated dict of packets.'''\n",
    "        dict_ = ct.defaultdict(list)\n",
    "        for i, caselet in enumerate(self.packets):\n",
    "            dict_[i] = caselet\n",
    "        \n",
    "        return dict_\n",
    "    \n",
    "    def to_list(self):\n",
    "        '''Return lists of packets.'''\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def info(self):\n",
    "        '''Return DataFrame of information per caselet.'''\n",
    "        pass\n",
    "    \n",
    "    \n",
    "```\n",
    "\n",
    "### Examples\n",
    "\n",
    "```python\n",
    "\n",
    "    Boilerplate\n",
    "    \n",
    "    >>> import lamana as la\n",
    "    >>> from lamana.input_ import Packets\n",
    "    >>> from lamana.models import Wilson_LT as wlt\n",
    "    >>> dft = wlt.Defaults()\n",
    "\n",
    "    Usage: A packet --> a future case and a singleplot (one axes)\n",
    "    \n",
    "    >>> packet = Packets(['400-200-800', '400-400-400'])\n",
    "    >>> packet\n",
    "    <lamana Packets object, `distribution`, size=1>\n",
    "    >>> packet()\n",
    "    {0:  ['400-200-800', '400-400-400']}\n",
    "    >>> case = la.distributions.Case(dft.load_params, dft.mat_props)\n",
    "    >>> case.apply(packet)\n",
    "    >>> singleplot = case.plot\n",
    "    >>> singleplot.naxes\n",
    "    1\n",
    "\n",
    "    Usage: Packets --> a group of cases (caselets) --> multiplot (n>1 axes)\n",
    "    \n",
    "    >>> packets = Packets([['400-200-800', '400-400-400'], ['400-0-1200']]) \n",
    "    >>> packets()\n",
    "    {0:  ['400-200-800', '400-400-400'],\n",
    "     1: ['400-0-1200']}\n",
    "    >>> cases = la.distributions.Cases(packets)            # assumes default parameters and properties    \n",
    "    >>> singleplot = case.plot\n",
    "    >>> singleplot.naxes\n",
    "    2\n",
    "    \n",
    "    Handling: if unequal plies are found, a new packet is generated automatically\n",
    "\n",
    "    >>> str_packets = [                                    # should be one caselet\n",
    "    ... '400-200-800', '400-400-400',                      # but nplies=5\n",
    "    ... '400-0-1200'                                       # and nplies=3; cannot plot together\n",
    "    ]\n",
    "    >>> packets = Packets(str_packets)\n",
    "    Using default distributions objects.\n",
    "    Unequal nplies found.  Separating...\n",
    "    >>> packets()\n",
    "    {0: ['400-200-800', '400-400-400'],\n",
    "     1: ['400-0-1200']}\n",
    "    >>> packets.size\n",
    "    2\n",
    "    >>> packets.nplies\n",
    "    [5, 3]                                                  # ordered by input position\n",
    "    >>> packets.info                                        # pandas DataFrame\n",
    "       nplies  p  contained\n",
    "    0  5       5  '400-200-800', '400-400-400'\n",
    "    1  3       5  '400-0-1200'\n",
    "    \n",
    "    Feature: For a distributions `Case` or `Cases` object --> stress distribution\n",
    "\n",
    "    >>> packets = Packets(['400-200-800', '400-400-400'], feature='distributions')\n",
    "    >>> packets\n",
    "    <lamana Packets object `distributions`, size=1>\n",
    "\n",
    "    Feature: For a predictions module object (NotImplemented) --> regression plot\n",
    "\n",
    "    >>> packets = Packets(['400-200-800', '400-400-400'], feature='predictions')\n",
    "    >>> packets\n",
    "    <lamana Packets object `predictions`, size=1>\n",
    "\n",
    "    Feature: For a ratios module (NotImplemented) --> layer group in a ratio plot\n",
    "\n",
    "    >>> packets = Packets(['400-200-800', '400-400-400'], feature='ratios')\n",
    "    >>> packets\n",
    "    <lamana Packets object `ratios`, size=1>\n",
    "    \n",
    "```\n",
    "\n",
    "\n",
    "### Vetting\n",
    "\n",
    "Benefits:\n",
    "    - This approach handles all analyses, conversions, validations, and reorderings (e.g. nply separation) of user input data.  \n",
    "    - It feeds a consistent form to `Case` and `Cases`\n",
    "    - Off loads the need to figure out what kind of caselet should be made.\n",
    "    - Preprocesses with light, strings and lists.\n",
    "    - Can later use in conjunction with a some startup functions e.g. `Start` to simplify user API.\n",
    "    - Handle future input types e.g. `GeoOrient` object.\n",
    "\n",
    "\n",
    "### Next Actions\n",
    "\n",
    "- Develop post 0.4.11.\n",
    "- Implement the General Convention strings. \n",
    "- Implement the ordering algorithms.\n",
    "- Implement the isvalid method.\n",
    "- Implement into `input_` module; refactor distributions `Case` and `Cases` to accept `Packets`.  Remove redundant code.\n",
    "\n",
    "\n",
    "### See Also\n",
    "\n",
    "- LPEP 003: original ideas on caselets\n",
    "- LPEP 007: plotting redesign\n",
    "\n",
    "\n",
    "### Copyright \n",
    "\n",
    "This document has been placed in the public domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPEP 009: Revisiting Entry Points \n",
    "\n",
    "- **Status: Draft**\n",
    "- **Type: Process**\n",
    "- **Date: April 07, 2016**\n",
    "- **Current Version: 0.4.11**\n",
    "\n",
    "\n",
    "### Motivation\n",
    "\n",
    "The user input can be complex and difficult to predict.  Additionaly, the user should not be bothered with the following:\n",
    "\n",
    "1. Worrying about which type to use as an entry point e.g. `Case` or `Cases`\n",
    "2. Remembering to `apply` as in `Case.apply`\n",
    "3. Worrying about particular signatures for each feature module.\n",
    "\n",
    "\n",
    "As feature modules are added, the entry points to LamAna increase while also broading the signature for caselets.  This broadening may become confusing over time.  The purpose of this proposal is to mitigate the user responsibility in setting up boilerpoint and focus on analysis.\n",
    "\n",
    "\n",
    "### Desired Ouptut\n",
    "\n",
    "After supplying caselet information, prompt the user with information it requires per feature module, e.g. load_params or mat_props.\n",
    "\n",
    "\n",
    "### Definitions\n",
    "\n",
    "\n",
    "\n",
    "### Specification\n",
    "\n",
    "\n",
    "\n",
    "### Examples\n",
    "\n",
    "```python\n",
    ">>> # Geometries to analyze\n",
    ">>> caselet = ['400-200-800', '400-400-400']\n",
    ">>> # What kind of analysis?\n",
    ">>> la.input_.Start(caselet, feature='distributions')\n",
    "... Please supply loading paramaters.  Press Enter to use defaults.\n",
    "... Please supply material properties.  Press Enter to use defaults.\n",
    "... Please supply laminate theory model.  Press Enter to use defaults.\n",
    "Using default load_params and mat_props...\n",
    "Using Wilson_LT model...\n",
    "Done.\n",
    "[<lamana Case object size=1, p=5>]\n",
    "```\n",
    "\n",
    "### Vetting\n",
    "\n",
    "\n",
    "\n",
    "### Next Actions\n",
    "\n",
    "- Design an object that routes user to specific feature module objects and prompts for necessary data.\n",
    "\n",
    "\n",
    "### See Also\n",
    "\n",
    "\n",
    "### Copyright \n",
    "\n",
    "This document has been placed in the public domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPEP 010: Decoupling `LaminateModels` from `Laminate`  \n",
    "\n",
    "\n",
    "- **Status: Draft**\n",
    "- **Type: Standards Track**\n",
    "- **Date: May 30, 2016**\n",
    "- **Current Version: 0.4.11**\n",
    "\n",
    "\n",
    "### Motivation\n",
    "\n",
    "The `LaminateModel` object is not a class, but it is rather a DataFrame object assigned to an instance attribute of the `Laminate` class.  The implementation was originally intended to reduce class objects creation (reducing memory), encourage simplicity and likely reduce the number of looping operations for populating DataFrame rows and columns.  However, this implicit architecture of the clandestine `LaminateModels` can lead to misunderstanding when trying to track the flow of objects.  In addition, during refactoring the `theories` objects, passing a pure `Laminate` object into the `theories.handshake()` has proven is impossible at the moment.  \n",
    "\n",
    "In effort to access separate objects and for clarity, this proposal maps out a plan to decouple `LaminateModel` from `Laminate` as a seprate object through subclassing.\n",
    "\n",
    "\n",
    "### Desired Ouptut\n",
    "\n",
    "A `LaminateModel` object that inherits from `Laminate` and `Stack`.\n",
    "\n",
    "\n",
    "### Specification\n",
    "\n",
    "1. Given a `FeatureInput`, create `LaminateModel`.\n",
    "2. If Exception raised, return a `Laminate` object.\n",
    "3. Update tests expecting `Laminate` to return `LaminateModel`\n",
    "4. Duplicate `_build_laminate` to `_build_primitive`; merge former with Stage 2. \n",
    "\n",
    "The latter objects should be achieved by extracting Stage 3 `_update_calucations` into `LaminateModel`.  For cleanup, we can separate object parsing attributes into their associated objects.  We can then serially call lower level objects to make the final product.\n",
    "\n",
    "`LaminateModel(Laminate(Stack))`\n",
    "\n",
    "\n",
    "### Examples\n",
    "\n",
    "```python\n",
    "# Create Separate Objects\n",
    ">>> S = la.constructs.Stack(FI)\n",
    ">>> S\n",
    "<Stack object>\n",
    ">>> L = la.contstructs.Laminate(FI)\n",
    ">>> L\n",
    "<Laminate object 400-[200]-800>\n",
    ">>>LM = la.constructs.LaminateModel(FI)\n",
    ">>> LM\n",
    "<LaminateModel object 400-[200]-800>\n",
    "\n",
    ">>> # Attribute Inheritance\n",
    ">>> S_attrs = ['stack_order', 'nplies', 'name', 'alias']\n",
    ">>> all([hasattr(S, attr) for attr in S_attrs])\n",
    "True\n",
    ">>> L_attrs = ['FeatureInput', 'Stack', 'Snapshot', 'LFrame']\n",
    ">>> all([hasattr(L, attr) for attr in ''.join([L_attrs, S_attrs])\n",
    "True\n",
    ">>> LM_attrs = ['LMFrame']\n",
    ">>> all([hasattr(LM, attr) for attr in ''.join([LM_attrs, L_attrs, S_attrs])\n",
    "True\n",
    "\n",
    "```\n",
    "\n",
    "### Vetting\n",
    "\n",
    "\n",
    "\n",
    "### Next Actions\n",
    "\n",
    "- Reduce object recreation; notice a FI is passed to `Stack` and `Laminate`.\n",
    "- Get image of how objects are passed prior to refactoring.\n",
    "\n",
    "### See Also\n",
    "\n",
    "- LPEP 004: Refactoring Stack to optimize object creation\n",
    "- LPEP 006: Defining objects\n",
    "\n",
    "\n",
    "### Copyright \n",
    "\n",
    "This document has been placed in the public domain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPEP 010: `Lexer`/`Parser`/`Resolver` Processing of geometry strings \n",
    "\n",
    "\n",
    "- **Status: Draft**\n",
    "- **Type: Standards Track**\n",
    "- **Date: Sept 10, 2016**\n",
    "- **Current Version: 0.4.13.dev**\n",
    "\n",
    "\n",
    "### Motivation\n",
    "\n",
    "A geometry string is the initial and most ciritcal object for defining a laminate construct.  It is provided by the user and must be parsed, validated and converted to a standardized data structure prior to building `Laminate`-like objects.  \n",
    "\n",
    "Since parsing geometry strings is executed everytime, this process needs to be efficient and fast as it affects runtime and testing performance.  Currently:\n",
    "\n",
    "- Runtime performance for building a Laminate object is on the order of milliseconds\n",
    "- Running 300+ tests is on the order of 10 minutes.\n",
    "\n",
    "We propose gutting the current parsing architecture and replacing it with a canonical lexical-parser approach (as used in traditional interpreters) using `rply`.  In place of an AST, we will implement a \"resolver\", which translates the parsed stream into current LamAna data types.\n",
    "\n",
    "We expect cleaner code and improved overall performance.\n",
    "\n",
    "\n",
    "### Desired Ouptut\n",
    "\n",
    "A lexer is a tool that tokenizes words, symbols, numbers in a string.  These tokens are parsed to compose a grammar that communicates within a language.  This parsed stream is typically fed into an Abstract Syntax Tree (AST).  These steps are common practice for making an interpreter for a programming language, but we are applying a modified approach to LamAna.\n",
    "\n",
    "###### Summarized Workflow\n",
    "\n",
    "We summarized the `Lexer`/`Parser`/`Resolver` workflow below.  Similar LamAna objects are color-coded and will be described later.\n",
    "\n",
    "![string processing](../docs/_images/string_processing.png)\n",
    "\n",
    "1. A user-defined geometry string is given\n",
    "1. The geostring is lexed and parsed into a generator that streams converted Pythonic types, i.e. `float`, `list`, `tuple`, `str`.  For inspection, the generator can be consumed by a builtin e.g. `list()`, `tuple()`.  The offical consumer is a `namedtuple`.\n",
    "1. The parser stream is resolved into an unfolded, linear stream of recomputed thickness values.  This result can either be *symmetric* or *assymmetric* about the central value(s).  The `Resolver` can be implemented with methods to output this stream in any desired format.\n",
    "\n",
    "###### Regarding Native `rply` Output\n",
    "\n",
    "The decision to use `rply` was inspired by Alex Gaynor's [talk](https://youtu.be/LCslqgM48D4?).  Typically, the lexer feeds a parser which feeds an AST. Given a geometry string `\"400-200-800\"`, we get the following output from `rply`:\n",
    "\n",
    "```\n",
    "[Token('NUMBER', '400'),\n",
    " [Token('LBRACKET', '['),\n",
    "  (Token('NUMBER', '400'), Token('COMMA', ',')),\n",
    "  Token('RBRACKET', ']')],\n",
    "  Token('NUMBER', '800')]\n",
    "```\n",
    "\n",
    "Unlike an AST, we don't need a nested tree of lists for building a tree of nodes.  We prefer a flat iterable of converted tokens.  We thus need to resolve this native output into a desired form.  Prior to this, we must modify the parser to clean up the stream.\n",
    "\n",
    "###### Expected `lamana` Output\n",
    "\n",
    "Given a geometry string `\"400-200-400S\"`, we will *parse* a native `rply` output and build a precursor `GeometryTuple` of python floats, lists, strings or tuples, e.g.\n",
    "\n",
    "                  GeometryTuple(400.0, [200.0], 400.0, \"S\")\n",
    "\n",
    "We will *resolve* the `Parser` stream into a decoded (or \"unfolded\"), linear order of layers.  This decoded object is a sub-component of a `Stack` object, which undergirds the structure of all sub-classed `Laminate`-like objects.\n",
    "\n",
    "                            ((\"outer\", 400.0),\n",
    "                             (\"inner\", 200.0),\n",
    "                             (\"middle\", 800.0),\n",
    "                             (\"inner\", 200.0),\n",
    "                             (\"outer\", 400.0))\n",
    "                             \n",
    "Notice the key characteristics of a `Resolver` object comprise an ordered collection of named layers and thickness values.  In addition, the middle value was recomputed due to the symmetry flag in the parser stream.\n",
    "\n",
    "\n",
    "### Definitions/Keywords\n",
    "\n",
    "Syntactically, we need to set up rules (a.k.a. \"procedures\") for a parser. We will use [BNF-like notations](http://digital.ni.com/public.nsf/allkb/83894C96C792DA9E86256C36005D188F) for geostring tokens:\n",
    "\n",
    "    <layer_ ::= <number>\n",
    "    \n",
    "    <non_duple> ::= <number>\n",
    "\n",
    "    <duple> ::= \"(\" <layer_1> \",\" <layer_2> \")\"; <layer_1> != <layer_2> \n",
    "    \n",
    "    <flag> ::= <FLAG>\n",
    "    \n",
    "    <middle> ::= <layer_>\n",
    "       \n",
    "    <inner> ::=  <non_duple>\n",
    "              | <duple>\n",
    "              | <non_duple> \",\"\n",
    "              | <duple> \",\"\n",
    "            \n",
    "    <inner_i> ::= <inner>\n",
    "                | <inner_i> <inner>\n",
    "\n",
    "    <inners> ::= <non_duple>\n",
    "               | \"[\" <inner_i> \"]\"\n",
    "\n",
    "    <outer> ::= <non_duple>\n",
    "              | <duple>\n",
    "             \n",
    "    <geostring> ::= <outer> \"-\" <inners> \"-\" <middle>\n",
    "                  | <outer> \"-\" <inners> \"-\" <middle> <flag>\n",
    "\n",
    "![generic geostrings](./_images/geometry_string.png)                             \n",
    "                             \n",
    "We will now clearly define the latter tokens \n",
    "\n",
    "- `layer_`: thickness for one layer\n",
    "- `duple`: **d**ual-layer, t**uple** of *unequal* layer thicknesses; left to right, tensile to compressive\n",
    "- `non-duple`: dual-layer ~~tuple~~ of *equal* layer thicknesses, e.g. **400**-[**200**]-800\n",
    "- flag: the final letter in a geometry string\n",
    "- middle: middle layer thickness\n",
    "- inner: inner layer thickness, component of `inners` (a.k.a `inner_i`).\n",
    "- inner_i: an (ith) inner or growing list of inner items. This is a defined in order to programatcially setup up recursion in the parser.\n",
    "- inners: a `list` of inner layer thicknesses; shown as brackets containing number(s).\n",
    "- outer: outer thickness\n",
    "- geostring: a geometry string in outer-inners-middle format\n",
    "\n",
    "Definitions in  LPEP 006 will need to be updated.\n",
    "\n",
    "### Design\n",
    "\n",
    "We are using `rply` and here is [an example on the project repo](https://github.com/alex/rply).  The tool can help make lexers and parsers.\n",
    "We demonstrated a simple, standard geometry string earlier.  Now we will parse a more complex, but generic geometry string that includes **duples**:\n",
    "\n",
    "                '(300, 100.)-[50, (25,75), 50]-800.0S'\n",
    "                    \n",
    "                    A                B          C   FLAG \n",
    "    \n",
    "This generic string contains numbers, a letter, symbols and groups of tokens that represent other tokens, including *inners* and *duples* enclosed by brackets and parentheses respectively.\n",
    "\n",
    "**Problem**: Given a geometry string (some comprising duples), how do we universally and efficiently make a linear collection of converted, layer thickness values?\n",
    "\n",
    "###### Hairpin Approach\n",
    "\n",
    "Let's take a step back and describe one approach.  We define a hairpin as a linear model of objects that *folds* on itself (there may be a more technical name for this, but hairpin is concise).  Given a geometry string of format `A - B - C` comprising `outer-inner-middle`, an abstract hairpin looks as follows:\n",
    "\n",
    "    i   Hairpin                 Deque-style             Notes\n",
    "    -   -------             --------------------        -----\n",
    "    0      C                         C                  # reverse iterate\n",
    "          / \\  \n",
    "    1    B   B'                  B - C - B              # append/-left\n",
    "         |   | \n",
    "    2    A   A'             A -  B - C - B' - A'        # append/-left\n",
    "\n",
    "Programatically, a *hairpin approach* instructs how to transform the geometry string format into a collection of objects, walking from `A --> B --> C --> B' ---> A'` if unfolded.  Note, the prime symbol indicates a similar layer type but potentially different numeric value.\n",
    "\n",
    "Some approaches for traversing this geostring are as follows:\n",
    "\n",
    "1. Intuitive approach - to iterate over a the geometry string, followed by a reversed iteration, popping off values.  \n",
    "2. Hairpin approach - use a double-ended queue, `deque`.  For simiplicity, we reverse iterate starting at `C`.  For each following iteration step (`i > 0`),  `appendleft` and `append` items to the deque.\n",
    "\n",
    "We submit that the second option is cleaner, more clear and eliminates an extra iteration step that reduces `O(n)` performance degradation. \n",
    "\n",
    "###### Components\n",
    "\n",
    "- `Lexer`: tokenize a geometry string\n",
    "- `Parser`: convert the stream to `yield` form: `A - [B] - C - FLAG`\n",
    "- `Resolver`: convert numerics to floats, resolve flags and unfold parsed stream to `yield` form `A -  B - C - B' - A'`\n",
    "\n",
    "### Specification\n",
    "\n",
    "###### Dependencies\n",
    "\n",
    "As mentioned, we use `rply`.  \n",
    "\n",
    "We will need to update the shared REGEXES dictionary in the LamAna library.\n",
    "\n",
    "```\n",
    "    REGEXES = {\n",
    "        ...\n",
    "        # 300, 300., 300.0\n",
    "        'number': r'(\\d+\\.*\\d*)',\n",
    "        # (\n",
    "        'lparen': r'\\(',\n",
    "        # )\n",
    "        'rparen': r'\\)',\n",
    "        # (\n",
    "        'lbracket': r'\\[',\n",
    "        # (\n",
    "        'rbracket': r'\\]',\n",
    "        # ,\n",
    "        'comma': r'\\,',\n",
    "        # -\n",
    "        'dash': r'-',\n",
    "        # \n",
    "        'whitespace': r'\\s+',\n",
    "        # ...800S --> S\n",
    "        'flag': r'[a-zA-Z]{1}$'     \n",
    "```\n",
    "\n",
    "###### `Lexer` Details\n",
    "\n",
    "The `Lexer` implementation will be generated based on these REGEXES.  This class inherits from `rply.LexerGenerator`.  Calling the `.lex()` method will yield a generator, which if consumed, returns a typical `rply`, pre-AST, nested output.  This generator stream is fed into a `Parser` object.\n",
    "\n",
    "###### `Parser` Details\n",
    "\n",
    "A `Parser` object inherits from `rply.ParserGenerator`.  This class contains a number of functions that parse the raw output and clean up the desired output.  This is accomplished by defining sets of rules or \"productions\" that essentially say, \"rule name: if the lexer signature has this format, use this functon\".  You can define different rules for the same rule name to cover different cases by making new functions and defining a new `@pg.productions` decorator.  The stream of lexer objects gets parsed in this manner and processed by instructions in the function. Learn how to apply productions in detail from Gaynor's [PyCon talk](https://youtu.be/LCslqgM48D4?t=14m31s).\n",
    "\n",
    "When consumed by a `namedtuple`, a `Parser` object should yield the equivalent of a `GeometryTuple`.\n",
    "\n",
    "**Caveat**: the details of how the productions work and are shared within the class namespace is a bit magical and mystical.  Explore those details in the [`rply` codebase](https://github.com/alex/rply/blob/master/rply/parsergenerator.py).\n",
    "\n",
    "###### Resolver\n",
    "\n",
    "This is where our design deviates from canon.  In place of an AST, we post-process the parser stream into an ordered, linear model of named layer thicknesses.  The result invokes the *hairpin approach* to tranform the parser stream.  The user/Dev passes a geometry string to a `Resolver` method, which invokes the Parser/Lexer objects.  Therefore, **we can simply use a `Resolver` object to lex, parse and resolve and geometry string**.\n",
    "\n",
    "When consumed by `tuple()` builtin, a `Resolver` object should yield the equivalent of an unfolded stack, i.e. the result of `decode_geometry()`.\n",
    "\n",
    "### Examples\n",
    "\n",
    "```python\n",
    "\n",
    "# Parsing a geometry string\n",
    ">>>p = Parser()\n",
    ">>>pg = p.parse(\"400-[200]-800\")\n",
    ">>>pg\n",
    "<generator object Parser.geostring at 0x000000000A140BF8>\n",
    ">>> tuple(pg)\n",
    "(400.0, [200.0], 800.0, None)\n",
    ">>> pg = p.parse(\"400-[200]-400S\")\n",
    ">>> tuple(pg)\n",
    "(400.0, [200.0], 400.0, \"S\")\n",
    "\n",
    "\n",
    "# Resolving a geometry string; internally resolving a parser stream\n",
    ">>> r = Resolver()\n",
    ">>> r.resolve(\"400.-[150.0,50]-400S\")         \n",
    "((\"outer\", 400.0),\n",
    " (\"inner\", 150.0),\n",
    " (\"middle\", 800.0),\n",
    " (\"inner\", 50.0),\n",
    " (\"outer\", 400.0))\n",
    "\n",
    "```    \n",
    "\n",
    "### Vetting\n",
    "\n",
    "Using a `Lexer`/`Parser`/`Resolver` has the following benefits (here the compared source methods are `input_.Geometry._parse_geometry` and `constructs.Stack.decode_geometry`):\n",
    "\n",
    "- Consuming a `Parser.parse` method is 1000x faster than comparable source method.\n",
    "- Consuming a `Resolver.resolve` method is nearly 100x faster than a comparable source function.\n",
    "- Running `Resolver.resolve` method gives comparable results to the source method.\n",
    "- Running the latter methods give comparable result to source methods.\n",
    "- Cleaner, canoncial code\n",
    "- Centralizes code in `input_`\n",
    "- Uses a generator each; possibly less loops\n",
    "- Tokenizes, parses, `GeometryTuple`s, unfolds, duples, flags \n",
    "- `Lexer`/`Parser` replaces outdated functions and methods (0.4.13.dev):\n",
    "    - `input_.tokenize_geostring`\n",
    "    - `input_.Geometry._parse_geometry`\n",
    "    - `input_.Geometry.check_symmetry`\n",
    "    - `input_.Geometry.parse_inner`\n",
    "    - `input_.Geometry._to_gen_convention`\n",
    "    - `utils.plottools.process_inner_i`\n",
    "    - `utils.plottools._get_duples`\n",
    "    - `utils.plottools._get_non_duples`\n",
    "    - `utils.plottools._get_outer`\n",
    "    - `utils.plottools._get_inner_i`\n",
    "    - `utils.plottools._get_middle`\n",
    "    - `utils.plottools._unfold_geometry`\n",
    "    - `utils.plottools._unfold_geometry2`\n",
    "- `Resolver` replaces `Stack.decode_geometry`\n",
    "\n",
    "The disadvantages:\n",
    "\n",
    "- Relies on another dependency\n",
    "- Uses decorator \"magic\" in productions to talk to each other by strings\n",
    "- ~~The actual `Parser.parse` method is 3x-4x slower than `Geometry._parse_geometry`.~~\n",
    "- `Parser.parse` is 1.5x-2x slower than the source method.\n",
    "- The new methods are not faster as desired, only on par with source.\n",
    "\n",
    "\n",
    "### Next Actions\n",
    "\n",
    "@Beta Coding\n",
    "- Steps for implementing lexer/parser/resolver\n",
    "    - ~~Clarify definitions for non-/duples~~\n",
    "    - ~~Clarify \"hairpin\" substitute for AST~~\n",
    "    - ~~Build tests~~\n",
    "    - ~~Make `Lexer`, `Parser` and `Resolver` classes~~\n",
    "    - ~~Clean up code to print raw list tree~~\n",
    "    - ~~Deprecate `first`, `inside`, `last`~~\n",
    "    - ~~Regex last letter as `FLAG`~~\n",
    "    - ~~`yield` inplace of `return` results~~\n",
    "    - ~~Fix regex to look for last characters only~~\n",
    "\n",
    "@Pre-Implementation\n",
    "- Add error handling\n",
    "- Change `p` to `tkn`\n",
    "- Test performance of returning parsed strings; compare to source.\n",
    "- Cleanup `Resolver`; replace nested `try` blocks, remove flags and iterations\n",
    "- Ear-mark for 0.5.0.\n",
    "- Make an LPEP\n",
    "- Try to LRU `Parser`\n",
    "    \n",
    "@Post Implementation\n",
    "- Upgrading geostrings with duples will require updating BaseDefaults.\n",
    "- Add updated REGEXES to `utils.config`\n",
    "- Add updated REGEX link to `utils.references`\n",
    "- Add ValidationError to `lt_exceptions`\n",
    "- Change flag in `GeometryTuple` and all related tests.\n",
    "- Add definition for *assymetric*/*symetric*/ laminate vs. symmetry flag\n",
    "\n",
    "@Docs\n",
    "- update jargon of inner_i to inners\n",
    "- update jargon of symmetric, S to flag\n",
    "- Unify `geostring` vs `geo_string`\n",
    "\n",
    "\n",
    "### See Also\n",
    "\n",
    "- `input_.Geometry`: parsers to be replaced with `Lexer` and `Parser`\n",
    "- `constructs.Stack.decode_geometry`: to be replaces with `Resolver`\n",
    "- Scratchpad - Input Strings (beta).ipynb: original test code\n",
    "\n",
    "\n",
    "### Copyright \n",
    "\n",
    "This document has been placed in the public domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": "3",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
