{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "nbsphinx": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Run: 2016-07-29 11:25:28\n"
     ]
    }
   ],
   "source": [
    "# Hidden TimeStamp\n",
    "import time, datetime\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print('Last Run: {}'.format(st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A guide for testing code prior to submitting pull request.\n",
    "\n",
    "Testing LamAna occurs in two flavors:\n",
    "\n",
    "1. Unit-testing with nose\n",
    "2. Regression testing of API with Jupyter or  runipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing code with `nose`\n",
    "\n",
    "The current testing utility is `nose`.  From the root directory, you can test all files prepended with \"test_\" by running:\n",
    "\n",
    "    $ nosetests\n",
    "   \n",
    "There are three types of tests contained in the source `lamana` directory:\n",
    "\n",
    "1. module tests: normal test files located in the \"./tests\" directory\n",
    "1. model tests: test files specific for custom models, located in \"./models/tests\"\n",
    "1. controls: .csv files located \"./tests/controls_LT\"\n",
    "\n",
    "Models tests are separated to support an extensibile design for author contributions. This design enables authors to create models and tests together with a single pull request to the standard module directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for the `utils` module writes and removes temporary files in a root directory called \"export\".  If this directory does not exist, one will be created.  These test check that writing and reading of files are consistent.  Temporary files are prefixed with \"temp\", but should be removed by these test functions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "\n",
    "    The locations for tests may change in future releases."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note:\n",
    "\n",
    "    Running ``nosetests`` on all tests can take a long time (e.g. >10 minutes for ~200 tests).  Testing a specific test modules may be more effective by temporarily removing unchanged test modules prior to running this command.  Add these tests back after this micro-testing is satisfactory.  This is easily done with GitHub Windows \"discard changes\" for deleted files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control files\n",
    "\n",
    "\n",
    "LamAna maintains .csv files with expected data for different lamanate configurations.  These files are tested with the `test_controls` module.  This module reads each control file and parses information such as layer numbers, number of points per layer and geometry.  Control files are named by these variables.\n",
    "\n",
    "Controls files can be created manually, but it may be simpler to make and then edit a starter file.  This process can be expedited for multiple files by passing LaminateModels into the `utils.tools.write_csv()` function.  This function will create a csv file for every LaminateModel, which can be altered as desired and tested by copying into the \"lamana/tests/controls_LT\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Coverage\n",
    "\n",
    "We use the following tools and commands to assess test coverage.  `nose-cov` helps to combine coverage reports for sub-packages automatically.  The remaining flags will report missing lines for the source directory. \n",
    "\n",
    "```\n",
    "$ pip install coverage, nose-cov\n",
    "$ nosetests --with-cov --cov lamana\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```    \n",
    "$ nosetests --with-cov --cov-report term-missing --cov lamana\n",
    "```\n",
    "LamAna aims for the highest \"reasonable\" coverage for core modules.  A separate ideology must be developed for testing `output_` as plots are tricky to test fully. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tests\n",
    "\n",
    "Prior to a release, it is fitting to test API regression tests on any demonstration notebooks in a development virtual environment and release branch  (see docs/demo.ipynb).  These are notebooks that run code using the LamAna package.  We are referring to the Reg. Test sections of the folowing *Development-Release Cycle Workflow* (see **Release/Hotfix Phase** and **Deployment Phase**):\n",
    "\n",
    "![Development-Release Cycle Workflow](../docs/_images/Diagram_Development_Workflow_071916.png)\n",
    "\n",
    "A superficial method for regression testiing is to **run all notebook cells; if any fail, then a regression has occured** and requires resolving before release.  A more in-depth method would be to **compare all cells to a prior version; any changes indicates a possible regression**. \n",
    "\n",
    "### Testing Woes\n",
    "\n",
    "A major problem that plagues most packages is that dependencies can change in an adverse way,  beyond a package maintainer's control.  If a dependency fails to install, the package may fail as well.  Package deployment relies on a number of components working successfully:\n",
    "\n",
    "- the package has minimal bugs\n",
    "- dependencies do not conflict\n",
    "- independent deprecations in dependencies do not break the package\n",
    "- the package manager (e.g. `pip`) can resolve dependencies issues\n",
    "- dependency links are not broken\n",
    "- local compllers are installed\n",
    "\n",
    "Additionally, testing on a local development environment is very different from testing a package installed from pypi.  On another system devoid of your local packages and setup, behaviors may vary dramatically and possibly an installation.\n",
    "\n",
    "To catch this type of environment bug, we need to make clean, isolated environements comprising minimal dependencies that is *fairly consistent* between release cycles.\n",
    "\n",
    "### Tools for Regression Testing\n",
    "\n",
    "The following tools are proposed for regression testing Juypyter notebooks:\n",
    "\n",
    "- [nb_conda_kernels](https://github.com/Anaconda-Platform/nb_conda_kernels): create environments and kernelspec on the fly\n",
    "- [nbval](https://github.com/computationalmodelling/nbval): validate notebook cells run and are consistnet with a version prior to running. ([beta](https://github.com/computationalmodelling/nbval/issues/6))\n",
    "- [nbdime](https://github.com/jupyter/nbdime): perform diff/merges of notebooks (beta)\n",
    "\n",
    "We will discuss the beta options in future developments.\n",
    "\n",
    "\n",
    "#### Regression Tests with `nb_conda_kernels`\n",
    "\n",
    "We need consistent environments to test notebooks.  `nb_conda_kernels` is a conda extension that offers a useful solution to this problem.  This extension is simple to use and setup up.  It simply requires an conda yaml file with a minimum of two dependences listed, i.g. `python` and `ipykernels`.  It comes pre-installed with Anaconda > 4.1 and has the following pros and cons:\n",
    "\n",
    "Pros:\n",
    "\n",
    "- Builds a fresh conda enviromemnt with minimal, isolated dependencies\n",
    "- Automatically adds a kernelspec and dropdown menu kernel option in Jupyter\n",
    "- Automatically removes kernelspc and menu option after the environment is removed.\n",
    "\n",
    "Cons:\n",
    "\n",
    "- Adds dependencies for `ipykernel` that should be pinned\n",
    "- Updating pins can be tedious, hampering workflow efficiency\n",
    "\n",
    "\n",
    "We start by handcrafting a custom yaml file special for testing jupyter notebooks.  This file resides in your package and is maintained across package versions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "\n",
    "    It is not so imperative to have an updated yaml.  We just need one that works and is consistent most of the time.  If something breaks due to an updated sub-dependency, you will find out during the testing phases and may selectively update the file as needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample yaml file:\n",
    "\n",
    "```\n",
    "# environment_jupyter.yaml\n",
    "\n",
    "name: nbregtest                       # names conda env\n",
    "dependencies:\n",
    "- python=3.5.1=4\n",
    "- ipykernel=4.1.0=py35_0\n",
    "- ...                                 # other dependencies\n",
    "\n",
    "```\n",
    "\n",
    "The \"name\" parameter sets the enviroment name and the kernelspec name."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "\n",
    "    A way to determine the remaining pinned dependencies and local versions, start by copying your frozen `environment_py<version>.yaml` and rename it.  Compare this list to dependencies by running ``conda install ipykernal=<version>``.  Now in your new yaml file,  except for python, remove entries not listed in the dependency list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's setup the environment.  Given Anaconda > 4.1 is installed, and a yaml file is created with the \"name\" parameter \"nbregtest\", use the following commands:\n",
    "\n",
    "```bash\n",
    "$ conda env update -f environment_jupyter.yaml\n",
    "$ activate nbregtest\n",
    "$ conda env list                        # verify dependencies\n",
    "```\n",
    "\n",
    "Now you have an environment with `python` and `ipkernel` dependencies.  You are ready to install your package for testing.  You may wish to test from a clone in deveop/editable mode or from testpypi.\n",
    "\n",
    "```bash\n",
    "$ cd package/folder\n",
    "$ pip install -e .\n",
    "\n",
    "or\n",
    "\n",
    "$ pip install --verbose --extra-index-url https://testpypi.python.org/pypi lamana\n",
    "```\n",
    "\n",
    "You can fire up Jupyter notebook from any location or environment.  I prefer a new console pointing to my test directory.  Open new concole:\n",
    "\n",
    "```bash\n",
    "$ cd package/tests\n",
    "$ jupyter notebook\n",
    "$ # conda install failed dependencies if needed\n",
    "$ # run notebook tests\n",
    "$ # shutdown jupyter\n",
    "```\n",
    "\n",
    "In our orginial console shutdown your environment:\n",
    "\n",
    "```bash\n",
    "$ deactivate\n",
    "$ conda env remove -n nbregtest\n",
    "```\n",
    "\n",
    "You have now tested notebooks in a controlled environment (with minimal jupyter dependencies) that use your package.  The env/kernelspec has been automatically removed.\n",
    "\n",
    "\n",
    "#### Regression Tests with `nbval`\n",
    "\n",
    "`nbval` compares cell output pre- and post-running your notebook.  It works as plugin to `pytest`.  \n",
    "\n",
    "    > cd ./docs\n",
    "    > py.test --nbval demo.ipynb --sanitize-with nbval_sanitize.cfg\n",
    "\n",
    "Currently `nbval` cannot handle random output (e.g. dict, sets, timestamps), so a sanitization file exists containing regexes that substitute certain outputs.  See [documention](https://github.com/computationalmodelling/nbval/blob/master/documentation.ipynb) to configure this `..docs/nbval_sanitize.cfg` file. A list of regexes and links are found in the `config.py` and `references.py` files respectively.\n",
    "\n",
    "As of 0.4.13, coninuous regression testing is added to the unpinned CI builds.  (see the shippable yaml).  Therefore, regression test will be active throughout the post-dev cycles.  The following cell outputs are ignored and must be validated by inspection:\n",
    "\n",
    "- Memory addresses i.e. matplotlib figures\n",
    "- Ordered containers i.e. dicts, sets (contents inside curly braces)\n",
    "- Print output includes parentheses in py2.\n",
    "\n",
    "The demonstration notebook has been prepared to work with `nbval`.  It is designed to compare data output before and after each run.  Note: if the notebook is updated then synced upstream, only the updated cells will be compared; there is no session-to-session regression test.  To acconlish this, consider an pinned version of the API notebook.\n",
    "\n",
    "\n",
    "#### Regression Tests with `nbdime`\n",
    "\n",
    "*TBD*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": "2",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
