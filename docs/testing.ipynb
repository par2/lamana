{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "nbsphinx": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Run: 2016-07-06 00:31:39\n"
     ]
    }
   ],
   "source": [
    "# Hidden TimeStamp\n",
    "import time, datetime\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print('Last Run: {}'.format(st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A guide for testing code prior to submitting pull request.\n",
    "\n",
    "Testing LamAna occurs in two flavors:\n",
    "\n",
    "1. Unit-testing with nose\n",
    "2. Regression testing of API with Jupyter or  runipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing code with `nose`\n",
    "\n",
    "The current testing utility is `nose`.  From the root directory, you can test all files prepended with \"test_\" by running:\n",
    "\n",
    "    $ nosetests\n",
    "   \n",
    "There are three types of tests contained in the source `lamana` directory:\n",
    "\n",
    "1. module tests: normal test files located in the \"./tests\" directory\n",
    "1. model tests: test files specific for custom models, located in \"./models/tests\"\n",
    "1. controls: .csv files located \"./tests/controls_LT\"\n",
    "\n",
    "Models tests are separated to support an extensibile design for author contributions. This design enables authors to create models and tests together with a single pull request to the standard module directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for the `utils` module writes and removes temporary files in a root directory called \"export\".  If this directory does not exist, one will be created.  These test check that writing and reading of files are consistent.  Temporary files are prefixed with \"temp\", but should be removed by these test functions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "\n",
    "    The locations for tests may change in future releases."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note:\n",
    "\n",
    "    Running ``nosetests`` on all tests can take a long time (e.g. >10 minutes for ~200 tests).  Testing a specific test modules may be more effective by temporarily removing unchanged test modules prior to running this command.  Add these tests back after this micro-testing is satisfactory.  This is easily done with GitHub Windows \"discard changes\" for deleted files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control files\n",
    "\n",
    "\n",
    "LamAna maintains .csv files with expected data for different lamanate configurations.  These files are tested with the `test_controls` module.  This module reads each control file and parses information such as layer numbers, number of points per layer and geometry.  Control files are named by these variables.\n",
    "\n",
    "Controls files can be created manually, but it may be simpler to make and then edit a starter file.  This process can be expedited for multiple files by passing LaminateModels into the `utils.tools.write_csv()` function.  This function will create a csv file for every LaminateModel, which can be altered as desired and tested by copying into the \"lamana/tests/controls_LT\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Coverage\n",
    "\n",
    "We use the following tools and commands to assess test coverage.  `nose-cov` helps to combine coverage reports for sub-packages automatically.  The remaining flags will report missing lines for the source directory. \n",
    "\n",
    "```\n",
    "$ pip install coverage, nose-cov\n",
    "$ nosetests --with-cov --cov lamana\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```    \n",
    "$ nosetests --with-cov --cov-report term-missing --cov lamana\n",
    "```\n",
    "LamAna aims for the highest \"reasonable\" coverage for core modules.  A separate ideology must be developed for testing `output_` as plots are tricky to test fully. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tests\n",
    "\n",
    "Prior to a release, it is fitting to test API regression tests on any demonstration notebooks in a development virtual environment and release branch  (see docs/demo.ipynb).  These are notebooks that run code using the lamana package.  If the notebook cells fail, then a regression has occured and requires resolving before release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Dependency Regression\n",
    "\n",
    "Dependency changes are beyond a package maintainer's control.  If a dependency fails to install, the package may fail as well.  However, a successfully deployed package often relies on a number of components working:\n",
    "\n",
    "- the package has minimal bugs\n",
    "- dependencies do not conflict\n",
    "- independent deprecations in dependencies do not break the package\n",
    "- the package manager (e.g. `pip`) can resolve dependencies\n",
    "\n",
    "Testing in a development environment is very different from testing a package from pypi.  The development environment may have a number of sub-dependencies\n",
    "that  with are cross-required for other packages.  In other words, on another system devoid of such a setup, installation behaviors may vary dramatically, and possibly break and installation.\n",
    "\n",
    "To catch this type of bug, particularly for testing notebook regression, we need to make a clean environement with minimal dependencies that rarely change and is fairly consistent between release cycles.  I say \"fairly consistent\" because with backports and other sub-dependency updates, it is nearly impossible to pin all of jupyter's dependencies.  We need to pin as much as we can to limit cross dependency contamination.  \n",
    "\n",
    "Solution: this proposed workflow uses [`nb_conda_kernels`](https://github.com/Anaconda-Platform/nb_conda_kernels) to help rebuild a consistent jupyter enviroment, in which to test notebooks and closely mimic the behavior of a fresh installation.  This extension comes pre-installed with Anaconda 4.1.  It magically generates kernelspecs for easy access to enviroment kernels from the notebook dropdown menu.  This also implies the dependencies are isolated per environment, which is critical for reliably resting pypi builds. \n",
    "\n",
    "### Testing with testpypi\n",
    "\n",
    "We start by handcrafting a custom enviroment.yaml file with `python` for jupyter.  This file drifts between versions an is only updated as needed  All jupyter `notebook` dependencies are includes, and all entries are pinned. \n",
    "\n",
    "To determine the jupyter dependencies and the versions of the pinned files, you can start by copying your `enviromentent_py<version>.yaml` file, running `conda install notebook=<version>` and then remove unnecessary entries in the yaml file.  Here is an example of the least elements required to work with the Anaconda extension (this file may vary for different jupyter versions):\n",
    "\n",
    "```\n",
    "# environment_example.yaml\n",
    "name: nbregtest\n",
    "dependencies:\n",
    "- python=3.5.1=4\n",
    "- notebook=4.1.0=py35_0\n",
    "- ...                                 # other dependencies\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "\n",
    "    Since it is impossible to pin all jupyer dependencies, only update the yaml file as needed.  It is not so important to have an updated jupyter version.  We just need one that works consistently most of the time.  If something breaks due \n",
    "to an updated sub-dependency, you will find out during the testing phases and can selectively update the file as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given Anaconda > 4.1 is installed and a yaml file is created with the \"name\" parameter \"nbregtest\":\n",
    "\n",
    "```bash\n",
    "> conda env update -f environment_jupyter.yaml\n",
    "> activate nbregtest\n",
    "> pip install --verbose --extra-index-url https://testpypi.python.org/pypi lamana\n",
    "> jupyter notebook\n",
    "> # conda install failed dependencies if needed\n",
    "> # run notebook tests\n",
    "> # shutdown jupyter\n",
    "> deactivate\n",
    "> conda env remove -n nbregtest\n",
    "```\n",
    "\n",
    "Notice the name in the yaml sets the enviroment name and the kernelspec name.  Notebooks have been tested in a controlled environment (with minimal jupyter dependencies), and the env/kernelspec has been removed."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": "2",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
